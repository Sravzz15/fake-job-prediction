# Task A1: Train Logistic Regression with CountVectorizer (BoW) and with TF-IDF, compare metrics
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix

# --- Load your data ---
# df = pd.read_csv('your_data.csv')  # <-- replace with your data load
# Must have df['description'] and df['label']

X = df['description'].fillna('')
y = df['label']

# train/test split (stratify to preserve class balance)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    return {
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred, zero_division=0),
        'recall': recall_score(y_test, y_pred, zero_division=0),
        'f1': f1_score(y_test, y_pred, zero_division=0)
    }

# --- 1) Bag-of-Words pipeline ---
cv = CountVectorizer(stop_words='english', max_features=10000, ngram_range=(1,2))
X_train_cv = cv.fit_transform(X_train)
X_test_cv  = cv.transform(X_test)

lr_cv = LogisticRegression(max_iter=2000, solver='saga', random_state=42)
lr_cv.fit(X_train_cv, y_train)
metrics_cv = evaluate_model(lr_cv, X_test_cv, y_test)

# --- 2) TF-IDF pipeline ---
tfidf = TfidfVectorizer(stop_words='english', max_features=10000, ngram_range=(1,2))
X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf  = tfidf.transform(X_test)

lr_tfidf = LogisticRegression(max_iter=2000, solver='saga', random_state=42)
lr_tfidf.fit(X_train_tfidf, y_train)
metrics_tfidf = evaluate_model(lr_tfidf, X_test_tfidf, y_test)

# Print comparison
print("=== BoW (CountVectorizer) Logistic Regression ===")
print(metrics_cv)
print("\n=== TF-IDF Logistic Regression ===")
print(metrics_tfidf)

# Optional: print classification report for TF-IDF model
print("\nClassification report (TF-IDF model):\n")
print(classification_report(y_test, lr_tfidf.predict(X_test_tfidf), zero_division=0))

# Task B: Use the trained TF-IDF logistic model to get predicted probabilities and show top 5 highest fake-probability jobs
import pandas as pd
import numpy as np

# Assuming lr_tfidf, tfidf, X_test, y_test from previous cell
# If not, re-fit tfidf and lr_tfidf as in the previous block.

# Predict probabilities for the whole dataset or test set
# Here I'll create a DataFrame with test rows and predicted probabilities
X_test_df = X_test.reset_index(drop=True)
y_test_df = y_test.reset_index(drop=True)

probs = lr_tfidf.predict_proba(tfidf.transform(X_test_df))[:, 1]  # probability of class 1 = fake
results = pd.DataFrame({
    'description': X_test_df,
    'label': y_test_df,
    'predicted_proba': probs
})

# Get top 5 job descriptions with highest fake probability
top5 = results.sort_values('predicted_proba', ascending=False).head(5)
pd.set_option('display.max_colwidth', 400)
print(top5[['predicted_proba', 'label', 'description']])

# Optional: Save to CSV for manual inspection
# top5.to_csv('top5_most_suspicious_jobs.csv', index=False)

# Task C: Test TF-IDF with max_features = [1000, 5000, 10000] and report accuracy (and other metrics)
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

def evaluate_tfidf_with_maxfeat(max_feat):
    vect = TfidfVectorizer(stop_words='english', max_features=max_feat, ngram_range=(1,2))
    Xtr = vect.fit_transform(X_train)
    Xte = vect.transform(X_test)
    model = LogisticRegression(max_iter=2000, solver='saga', random_state=42)
    model.fit(Xtr, y_train)
    ypred = model.predict(Xte)
    return {
        'max_features': max_feat,
        'accuracy': accuracy_score(y_test, ypred),
        'precision': precision_score(y_test, ypred, zero_division=0),
        'recall': recall_score(y_test, ypred, zero_division=0),
        'f1': f1_score(y_test, ypred, zero_division=0)
    }

for m in [1000, 5000, 10000]:
    print(evaluate_tfidf_with_maxfeat(m))

